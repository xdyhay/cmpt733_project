{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13249,"status":"ok","timestamp":1648594489533,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"yvcztVrr5teC","outputId":"c81700f2-9a1b-47c9-80ee-771ce3170238"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /home/xdy/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/xdy/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /home/xdy/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import transformers\n","import torch\n","import matplotlib.pyplot as plt\n","import time\n","import datetime\n","\n","from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n","from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","\n","from utilize import gen_dataframe"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1648594501628,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"WW8U0-YL-mSM","outputId":"f270d045-f404-4825-c413-f8e92438a9ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU: 1 NVIDIA GeForce RTX 2080\n"]},{"data":{"text/plain":["device(type='cuda')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('GPU:', torch.cuda.device_count(), torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":9759,"status":"ok","timestamp":1648594499509,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"oB7tKlfN56hI","outputId":"9a0adf86-a09c-4371-cacc-bae34f120367"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>tweet</th>\n","      <th>cleaned_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2022-03-07 16:21:36</td>\n","      <td>@MinMinHugs @flat__stanley @mimi_soapbox @mrma...</td>\n","      <td>aunt heart attack survived sad dont course thi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2022-03-07 15:47:58</td>\n","      <td>@CBCToronto @fordnation &amp;amp; @celliottability...</td>\n","      <td>decision racially demeaning asian canadian bel...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2022-03-07 14:55:50</td>\n","      <td>Dr. Simone Gold #TrumpTerrorist #AntiVaxxer #H...</td>\n","      <td>simone gold 'guilty f'ck' trespassing inside c...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2022-03-07 11:54:39</td>\n","      <td>Another #antivaxxer who keeps whining about no...</td>\n","      <td>another keep whining nothing cmon relax enjoy ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2022-03-07 05:08:13</td>\n","      <td>@KyleJGlen And yet according to the #antivaxxe...</td>\n","      <td>yet according freedumb idiot isnt happening ni...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  date                                              tweet  \\\n","0  2022-03-07 16:21:36  @MinMinHugs @flat__stanley @mimi_soapbox @mrma...   \n","1  2022-03-07 15:47:58  @CBCToronto @fordnation &amp; @celliottability...   \n","2  2022-03-07 14:55:50  Dr. Simone Gold #TrumpTerrorist #AntiVaxxer #H...   \n","3  2022-03-07 11:54:39  Another #antivaxxer who keeps whining about no...   \n","4  2022-03-07 05:08:13  @KyleJGlen And yet according to the #antivaxxe...   \n","\n","                                       cleaned_tweet  \n","0  aunt heart attack survived sad dont course thi...  \n","1  decision racially demeaning asian canadian bel...  \n","2  simone gold 'guilty f'ck' trespassing inside c...  \n","3  another keep whining nothing cmon relax enjoy ...  \n","4  yet according freedumb idiot isnt happening ni...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = gen_dataframe('./data/')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ne9L780F575Z"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2679,"status":"ok","timestamp":1648544518702,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"hU6HpXOhjlYZ","outputId":"a72676da-1b24-4ed0-db76-4b5fba2ef082"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package names to /home/xdy/nltk_data...\n","[nltk_data]   Package names is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/xdy/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package state_union to /home/xdy/nltk_data...\n","[nltk_data]   Package state_union is already up-to-date!\n","[nltk_data] Downloading package twitter_samples to\n","[nltk_data]     /home/xdy/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package movie_reviews to\n","[nltk_data]     /home/xdy/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/xdy/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /home/xdy/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/xdy/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","nltk.download([\n","    \"names\",\n","    \"stopwords\",\n","    \"state_union\",\n","    \"twitter_samples\",\n","    \"movie_reviews\",\n","    \"averaged_perceptron_tagger\",\n","    \"vader_lexicon\",\n","    \"punkt\",\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12311,"status":"ok","timestamp":1648544551218,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"eVoh4S2vrdXu","outputId":"d15fc356-b2df-4722-82b7-fbaf7e26303e"},"outputs":[{"name":"stdout","output_type":"stream","text":["                  date                                              tweet  \\\n","0  2022-03-07 16:21:36  @MinMinHugs @flat__stanley @mimi_soapbox @mrma...   \n","1  2022-03-07 15:47:58  @CBCToronto @fordnation &amp; @celliottability...   \n","2  2022-03-07 14:55:50  Dr. Simone Gold #TrumpTerrorist #AntiVaxxer #H...   \n","3  2022-03-07 11:54:39  Another #antivaxxer who keeps whining about no...   \n","4  2022-03-07 05:08:13  @KyleJGlen And yet according to the #antivaxxe...   \n","\n","                                       cleaned_tweet  label  \n","0  aunt heart attack survived sad dont course thi...      0  \n","1  decision racially demeaning asian canadian bel...      1  \n","2  simone gold 'guilty f'ck' trespassing inside c...      1  \n","3  another keep whining nothing cmon relax enjoy ...      1  \n","4  yet according freedumb idiot isnt happening ni...      0  \n","        date  tweet  cleaned_tweet\n","label                             \n","0      15065  15065          15065\n","1       9911   9911           9911\n"]}],"source":["# pseudo-labelling\n","\n","def assign_labels(df):\n","    labels = []\n","    sia = SentimentIntensityAnalyzer()\n","    for _, row in df.iterrows():\n","        scores = sia.polarity_scores(row['tweet'])\n","        label = 0\n","        if scores['pos'] < scores['neg']:\n","            label = 1\n","        if scores['neg'] == 1:\n","            label = 1\n","        labels.append(label)\n","    df['label'] = labels\n","    return df\n","\n","df = assign_labels(df)\n","\n","print(df.head())\n","print(df.groupby(['label']).count())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ro1wVqef6e1O"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"x6CGwAMY6bdX"},"outputs":[],"source":["X = df['cleaned_tweet'].values\n","y = df['label'].values"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["bf00ad970cd149b2b2697e4b7a30d1c3","430365a3731346d48e471d1aa5860a1c","080c8e903958429d8d6c581d42adfcc4","cba186de775d40c2847baa6871993186","d3eeb384432441969ca8f9869943727a","9c49f0eafdc74606b2fb17834d5b2322","1e5b8ab22a6c4c838386f5f5e54feaed","460f756bee3c4fcbad67c0cc2e3824b2","cc526ee468a84c2aa968c92275202bdb","7b7a2648256a48c99485f5a3f3c7ccfe","96076490506f43d19deae274e330d561","172215f9e94f4f1b807bee980aaa08a3","6092d72c790545a2a0c46f3c380f4761","7044634fa7854b9587eec226b41b4f18","98f192dfb0074571b86e7442ad35f62a","19657aacbd1847739703cd3320c3d29b","5cdc2d6a98704039a2ebd24d9d497375","555f5d766ab548a2b2d6a4e977a85b56","9b8289cee46d4815957f5501b2b7b83c","458691e7cff74f22b441864819665e43","8fc91653f6a742b6a4f4c25aea99868c","6b8ca737380d466d933f54b07911d60d","fa443ba2f2e44eac95c20fc8f62cc04c","785f154ce2d04a69abef510a8bdd17cd","aef988d834f14908b9810307ff7c5f81","bceec7a6c20e46d885a30a7da14c07c3","41ca850b2d464febb37a5e3d64e30134","ac87751a263440008c8bd9b92f11aa96","9fc6d141dd6b460b95b0175cc31c3cc9","95c1e2220e20415b807117a18755cd46","0a2e5810dd3e4e49866fc53a03b41f19","5868d3ba363f482bad185afaaada64eb","ecdd435b709e43a0937a0db2454c6b5d"]},"executionInfo":{"elapsed":1597,"status":"ok","timestamp":1648544627016,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"k6RIBUzK6brd","outputId":"7b76c193-3914-407c-9a6f-b0cd4032ef24"},"outputs":[],"source":["# BERT Tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1648544729090,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"4lEVTWzC6b2g","outputId":"441c8999-b7c4-420d-d4bb-46ae0f10102e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: aunt heart attack survived sad dont course thing happen life unfortunately love though fit warped mind never called way\n","Tokenized: ['aunt', 'heart', 'attack', 'survived', 'sad', 'don', '##t', 'course', 'thing', 'happen', 'life', 'unfortunately', 'love', 'though', 'fit', 'warped', 'mind', 'never', 'called', 'way']\n","Token IDs: [5916, 2540, 2886, 5175, 6517, 2123, 2102, 2607, 2518, 4148, 2166, 6854, 2293, 2295, 4906, 25618, 2568, 2196, 2170, 2126]\n"]}],"source":["print('Original:', X[0])\n","print('Tokenized:', tokenizer.tokenize(X[0]))\n","print('Token IDs:', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X[0])))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12735,"status":"ok","timestamp":1648552376776,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"uFSKt2CI6jri","outputId":"b13314c2-ed65-4864-f984-f93d1da80821"},"outputs":[{"name":"stdout","output_type":"stream","text":["67\n"]}],"source":["# find maximum length\n","max_seq_len = 0\n","for text in X:\n","    input_ids = tokenizer.encode(text, add_special_tokens=True)\n","    max_seq_len = max(max_seq_len, len(input_ids))\n","print(max_seq_len)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15611,"status":"ok","timestamp":1648553149994,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"bSLGqKbu6j27","outputId":"c2ebcbc8-af42-4828-ec6a-8843e320e59c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/shared/CMPT/big-data/condaenv/gt/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# tokenize all text and map the tokens to word IDs\n","input_ids = []\n","attn_masks = []\n","\n","for text in X:\n","    encoded_dict = tokenizer.encode_plus(text,\n","                                         add_special_tokens=True, # add [CLS] & [SEP]\n","                                         max_length=max_seq_len,\n","                                         pad_to_max_length=True, # pad 0 up to max length\n","                                         return_attention_mask=True,\n","                                         return_tensors='pt')\n","    input_ids.append(encoded_dict['input_ids'])\n","    attn_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attn_masks = torch.cat(attn_masks, dim=0)\n","labels = torch.tensor(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1648553149995,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"UN7LaFHo-x6x","outputId":"f74146e0-c571-4a0d-d66b-236f92c1c8c2"},"outputs":[],"source":["# dataset = TensorDataset(input_ids, attn_masks, labels)\n","\n","# # train test split\n","# train_size = int(len(dataset) * 0.8)\n","# test_size = len(dataset) - train_size\n","# train_set, test_set = random_split(dataset, [train_size, test_size])\n","\n","# # train val split\n","# val_size = int(train_size * 0.25)\n","# train_size = train_size - val_size\n","# train_set, val_set = random_split(train_set, [train_size, val_size])\n","\n","# (len(train_set), len(test_set), len(val_set))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = TensorDataset(input_ids, attn_masks, labels)\n","\n","# training validation split\n","train_size = int(len(dataset) * 0.8)\n","val_size = len(dataset) - train_size\n","train_set, val_set = random_split(dataset, [train_size, val_size])\n","\n","(len(train_set), len(val_set))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yxUWaYHYCJI0"},"outputs":[],"source":["batch_size = 32\n","train_dataloader = DataLoader(train_set,\n","                              sampler=RandomSampler(train_set),\n","                              batch_size=batch_size)\n","val_dataloader = DataLoader(val_set,\n","                            sampler=SequentialSampler(val_set),\n","                            batch_size=batch_size)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2624,"status":"ok","timestamp":1648553155524,"user":{"displayName":"X Y","userId":"03026570408648051473"},"user_tz":420},"id":"PceJhwfLJZ7L","outputId":"58e5bd04-3d29-4e62-b1d1-b2411fa9ccbc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# train classification model\n","bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","bert_model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkQU6TDyO4RD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def format_time(t):\n","    time_rounded = int(round((t)))\n","    # hh:mm:ss\n","    return str(datetime.timedelta(seconds=time_rounded))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"9QGcL5rnO4Oe"},"outputs":[],"source":["lr = 2e-5\n","eps = 1e-8\n","n_epochs = 2\n","\n","save_model_path = 'model/save'\n","optimizer = torch.optim.AdamW(bert_model.parameters(), lr=lr, eps=eps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9PkycRGO4HS"},"outputs":[],"source":["# start training \n","\n","train_stats = {'Epoch':[], 'Loss':[], 'Accuracy':[]}\n","test_stats = {'Epoch':[], 'Loss':[], 'Accuracy':[]}\n","\n","for epoch in range(n_epochs):\n","    start_time = time.time()\n","\n","    print(f'Epoch:{epoch+1} / {n_epochs}')\n","    print('Training...')\n","    \n","    bert_model.train()\n","    loss = 0\n","    losses = []\n","    accuracy = 0\n","    accuracies = []\n","    \n","    for step, batch in enumerate(train_dataloader):\n","        if step % 100 == 0 and not step == 0:\n","            print(f'Batch {step} / {len(train_dataloader)}, time cost: {format_time(time.time() - start_time)}')\n","\n","        batch_ids = batch[0].to(device)\n","        batch_masks = batch[1].to(device)\n","        batch_labels = batch[2].to(device)\n","\n","        bert_model.zero_grad()\n","\n","        \"\"\"\n","        https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\n","\n","        Returns\n","\n","        loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Classification (or regression if config.num_labels==1) loss.\n","\n","        logits (torch.FloatTensor of shape (batch_size, config.num_labels)) — Classification (or regression if config.num_labels==1) scores (before SoftMax).\n","\n","        hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n","\n","        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n","\n","        attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n","        \"\"\"\n","\n","        outputs = bert_model(batch_ids,\n","                             token_type_ids=None,\n","                             attention_mask=batch_masks,\n","                             labels=batch_labels)\n","        loss = outputs.loss        \n","        logits = outputs.logits\n","\n","        losses.append(loss.item())\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = batch_labels.to('cpu').numpy()\n","\n","        accuracy = np.sum(np.argmax(logits, axis=1) == label_ids) / len(label_ids)\n","        accuracies.append(accuracy)\n","\n","        loss.backward()\n","        optimizer.step()\n","    \n","    train_stats['Epoch'].append(epoch+1)\n","    train_stats['Loss'].append(np.mean(losses))\n","    train_stats['Accuracy'].append(np.mean(accuracies))\n","\n","    print('Total time cost for training :', format_time(time.time() - start_time))\n","\n","    # torch.save(bert_model.state_dict(), save_model_path)\n","\n","    print()\n","    print('Validating...')\n","\n","    bert_model.eval()\n","    val_loss = 0\n","    val_losses = []\n","    val_accuracy = 0\n","    val_accuracies = []\n","    # n_val_steps = 0\n","\n","    for batch in val_dataloader:\n","        batch_ids = batch[0].to(device)\n","        batch_masks = batch[1].to(device)\n","        batch_labels = batch[2].to(device)\n","\n","        with torch.no_grad():\n","            outputs = bert_model(batch_ids,\n","                                 token_type_ids=None,\n","                                 attention_mask=batch_masks,\n","                                 labels=batch_labels)\n","            val_loss = outputs.loss\n","            val_logits = outputs.logits\n","            \n","            val_losses.append(val_loss.item())\n","            val_logits = val_logits.detach().cpu().numpy()\n","            val_label_ids = batch_labels.to('cpu').numpy()\n","\n","            val_accuracy = np.sum(np.argmax(val_logits, axis=1) == val_label_ids) / len(val_label_ids)\n","            val_accuracies.append(val_accuracy)\n","\n","    test_stats['Epoch'].append(epoch+1)\n","    test_stats['Loss'].append(np.mean(val_losses))\n","    test_stats['Accuracy'].append(np.mean(val_accuracies))\n","\n","    print()\n","    print('Finished Training.')\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test_stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plt.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# evaluation on test dataset\n","\n","# bert_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwCUd7MLMdP6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"model_bert2.ipynb","provenance":[]},"interpreter":{"hash":"e205a3cfba7d0b5e8dcdd494600077bf59bc672a4dd4b79e51c82dea3172ef3c"},"kernelspec":{"display_name":"py38-gt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{"080c8e903958429d8d6c581d42adfcc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_460f756bee3c4fcbad67c0cc2e3824b2","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc526ee468a84c2aa968c92275202bdb","value":231508}},"0a2e5810dd3e4e49866fc53a03b41f19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"172215f9e94f4f1b807bee980aaa08a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6092d72c790545a2a0c46f3c380f4761","IPY_MODEL_7044634fa7854b9587eec226b41b4f18","IPY_MODEL_98f192dfb0074571b86e7442ad35f62a"],"layout":"IPY_MODEL_19657aacbd1847739703cd3320c3d29b"}},"19657aacbd1847739703cd3320c3d29b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e5b8ab22a6c4c838386f5f5e54feaed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41ca850b2d464febb37a5e3d64e30134":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"430365a3731346d48e471d1aa5860a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c49f0eafdc74606b2fb17834d5b2322","placeholder":"​","style":"IPY_MODEL_1e5b8ab22a6c4c838386f5f5e54feaed","value":"Downloading: 100%"}},"458691e7cff74f22b441864819665e43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"460f756bee3c4fcbad67c0cc2e3824b2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"555f5d766ab548a2b2d6a4e977a85b56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5868d3ba363f482bad185afaaada64eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cdc2d6a98704039a2ebd24d9d497375":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6092d72c790545a2a0c46f3c380f4761":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cdc2d6a98704039a2ebd24d9d497375","placeholder":"​","style":"IPY_MODEL_555f5d766ab548a2b2d6a4e977a85b56","value":"Downloading: 100%"}},"6b8ca737380d466d933f54b07911d60d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7044634fa7854b9587eec226b41b4f18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b8289cee46d4815957f5501b2b7b83c","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_458691e7cff74f22b441864819665e43","value":28}},"785f154ce2d04a69abef510a8bdd17cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac87751a263440008c8bd9b92f11aa96","placeholder":"​","style":"IPY_MODEL_9fc6d141dd6b460b95b0175cc31c3cc9","value":"Downloading: 100%"}},"7b7a2648256a48c99485f5a3f3c7ccfe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fc91653f6a742b6a4f4c25aea99868c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95c1e2220e20415b807117a18755cd46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96076490506f43d19deae274e330d561":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98f192dfb0074571b86e7442ad35f62a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fc91653f6a742b6a4f4c25aea99868c","placeholder":"​","style":"IPY_MODEL_6b8ca737380d466d933f54b07911d60d","value":" 28.0/28.0 [00:00&lt;00:00, 647B/s]"}},"9b8289cee46d4815957f5501b2b7b83c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c49f0eafdc74606b2fb17834d5b2322":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fc6d141dd6b460b95b0175cc31c3cc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac87751a263440008c8bd9b92f11aa96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aef988d834f14908b9810307ff7c5f81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95c1e2220e20415b807117a18755cd46","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a2e5810dd3e4e49866fc53a03b41f19","value":570}},"bceec7a6c20e46d885a30a7da14c07c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5868d3ba363f482bad185afaaada64eb","placeholder":"​","style":"IPY_MODEL_ecdd435b709e43a0937a0db2454c6b5d","value":" 570/570 [00:00&lt;00:00, 10.0kB/s]"}},"bf00ad970cd149b2b2697e4b7a30d1c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_430365a3731346d48e471d1aa5860a1c","IPY_MODEL_080c8e903958429d8d6c581d42adfcc4","IPY_MODEL_cba186de775d40c2847baa6871993186"],"layout":"IPY_MODEL_d3eeb384432441969ca8f9869943727a"}},"cba186de775d40c2847baa6871993186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b7a2648256a48c99485f5a3f3c7ccfe","placeholder":"​","style":"IPY_MODEL_96076490506f43d19deae274e330d561","value":" 226k/226k [00:00&lt;00:00, 933kB/s]"}},"cc526ee468a84c2aa968c92275202bdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3eeb384432441969ca8f9869943727a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecdd435b709e43a0937a0db2454c6b5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa443ba2f2e44eac95c20fc8f62cc04c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_785f154ce2d04a69abef510a8bdd17cd","IPY_MODEL_aef988d834f14908b9810307ff7c5f81","IPY_MODEL_bceec7a6c20e46d885a30a7da14c07c3"],"layout":"IPY_MODEL_41ca850b2d464febb37a5e3d64e30134"}}}}},"nbformat":4,"nbformat_minor":0}
